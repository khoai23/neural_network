{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_string(list_of_strings):\n",
    "    longest_string = None\n",
    "    longest_string_len = 0     \n",
    "    for s in list_of_strings:\n",
    "        if len(s) > longest_string_len:\n",
    "            longest_string_len = len(s)\n",
    "            longest_string = s    \n",
    "    return longest_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 363 µs\n"
     ]
    }
   ],
   "source": [
    "list_of_strings = ['abc', 'python', 'dima']\n",
    "%time max_length = print(find_longest_string(list_of_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 350 µs\n"
     ]
    }
   ],
   "source": [
    "large_list_of_strings = list_of_strings*1000\n",
    "%time print(find_longest_string(large_list_of_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 32 ms, total: 21.2 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "large_list_of_strings = list_of_strings*100000000\n",
    "%time max_length = max(large_list_of_strings, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# step 1:\n",
    "list_of_string_lens = (len(s) for s in large_list_of_strings)\n",
    "list_of_string_lens = zip(large_list_of_strings, list_of_string_lens)\n",
    "#step 2:\n",
    "max_len = max(list_of_string_lens, key=lambda t: t[1])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = len\n",
    "def reducer(p, c):\n",
    "    if p[1] > c[1]:\n",
    "        return p\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from functools import reduce\n",
    "#step 1\n",
    "mapped = map(mapper, large_list_of_strings)\n",
    "mapped = zip(large_list_of_strings, mapped)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install chunkify #Not working\n",
    "def chunkify(_list, number_of_chunks=8):\n",
    "    chunk_size = len(_list) // number_of_chunks\n",
    "    # return list\n",
    "    # return [ _list[i*chunk_size:(i+1)*chunk_size] for i in range(number_of_chunks) ]\n",
    "    # return iterator\n",
    "    for i in range(number_of_chunks):\n",
    "        yield _list[i*chunk_size:(i+1)*chunk_size]\n",
    "    raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "data_chunks = chunkify(large_list_of_strings, number_of_chunks=36)\n",
    "#step 1:\n",
    "reduced_all = []\n",
    "for chunk in data_chunks:\n",
    "    mapped_chunk = map(mapper, chunk)\n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    \n",
    "    reduced_chunk = reduce(reducer, mapped_chunk)\n",
    "    reduced_all.append(reduced_chunk)\n",
    "    \n",
    "#step 2:\n",
    "reduced = reduce(reducer, reduced_all)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def chunks_mapper(chunk):\n",
    "    mapped_chunk = map(mapper, chunk) \n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    return reduce(reducer, mapped_chunk)\n",
    "\n",
    "data_chunks = chunkify(large_list_of_strings, number_of_chunks=30)\n",
    "#step 1:\n",
    "mapped = map(chunks_mapper, data_chunks)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(8)\n",
    "data_chunks = chunkify(large_list_of_strings, number_of_chunks=8)\n",
    "#step 1:\n",
    "mapped = pool.map(chunks_mapper, data_chunks)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups()\n",
    "data = news.data*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "def clean_word(word):\n",
    "    return re.sub(r'[^\\w\\s]','',word).lower()\n",
    "\n",
    "def word_not_in_stopwords(word):\n",
    "    return word not in ENGLISH_STOP_WORDS and word and word.isalpha()    \n",
    "    \n",
    "def find_top_words(data):\n",
    "    cnt = Counter()\n",
    "    for text in data:\n",
    "        tokens_in_text = text.split()\n",
    "        tokens_in_text = map(clean_word, tokens_in_text)\n",
    "        tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "        cnt.update(tokens_in_text)\n",
    "        \n",
    "    return cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 260 ms, total: 1min 31s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('subject', 122520),\n",
       " ('lines', 118240),\n",
       " ('organization', 111850),\n",
       " ('writes', 78360),\n",
       " ('article', 67540),\n",
       " ('people', 58320),\n",
       " ('dont', 58130),\n",
       " ('like', 57570),\n",
       " ('just', 55790),\n",
       " ('university', 55440)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time find_top_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(text):\n",
    "    tokens_in_text = text.split()\n",
    "    tokens_in_text = map(clean_word, tokens_in_text)\n",
    "    tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "    return Counter(tokens_in_text)\n",
    "def reducer(cnt1, cnt2):\n",
    "    cnt1.update(cnt2)\n",
    "    return cnt1\n",
    "def chunk_mapper(chunk):\n",
    "    mapped = map(mapper, chunk)\n",
    "    reduced = reduce(reducer, mapped)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of cpus: 20\n",
      "[('subject', 122492), ('lines', 118210), ('organization', 111822), ('writes', 78340), ('article', 67524), ('people', 58305), ('dont', 58113), ('like', 57549), ('just', 55779), ('university', 55427)]\n",
      "CPU times: user 1.61 s, sys: 288 ms, total: 1.9 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import reduce\n",
    "print(\"Num of cpus: %d\" % cpu_count())\n",
    "pool = Pool(8)\n",
    "\n",
    "data_chunks = chunkify(data, number_of_chunks=36)\n",
    "#step 1:\n",
    "mapped = pool.map(chunk_mapper, data_chunks)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
