{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  6 13:08:28 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "|  0%   39C    P2    52W / 250W |    859MiB / 11177MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     13201      C   python3                           857MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        # additional state condenser. 2*2 is bidirectional * (cell+hidden), reduce down to hidden_dim\n",
    "        self.state_condenser = nn.Linear(2*2*hidden_dim, hidden_dim)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        # LSTM take input as [length, batch, emb], which is already formatted in BucketIterator\n",
    "        lstm_seq, lstm_state = self.lstm(embeds)\n",
    "        hidden_state, cell_state = lstm_state\n",
    "        # states are both [2, batch_size, hidden_dim], cat + transpose\n",
    "        batch_size = lstm_seq.shape[1]\n",
    "        condensed_state = self.state_condenser(torch.cat(lstm_state, dim=0).transpose(0, 2)\n",
    "                                                   .contiguous().view(batch_size, 2*2*self.hidden_dim))\n",
    "        # apply dropout\n",
    "        condensed_state = self.dropout(condensed_state)\n",
    "        lstm_seq = self.dropout(lstm_seq)\n",
    "        return condensed_state, lstm_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayeredOutput(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super(LayeredOutput, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.internal_layer = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, inputs, num_apply_layer=3):\n",
    "        # input in form of [batch_size, hidden_dim]\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = inputs.view(batch_size, 1, self.hidden_dim)\n",
    "        outputs = [inputs]\n",
    "        for i in range(num_apply_layer):\n",
    "            next_output = self.internal_layer(outputs[-1]).view(batch_size, -1, self.hidden_dim)\n",
    "            outputs.append(next_output)\n",
    "        # output in form of [batch_size, 2^n-1, hidden_dim], need to transpose to fit BucketIterator form\n",
    "        return torch.cat(outputs, dim=1).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_layer, self.v_layer = [nn.Linear(hidden_dim * 2, hidden_dim) for _ in range(2)]\n",
    "    \n",
    "    def forward(self, querries, keys, mask=None):\n",
    "        # querries in [2^n-1, batch_size, hidden_dim], transpose them back first, linear the last dimension\n",
    "        # keys should be [src_len, batch_size, hidden_dim] already, so we transpose keys to [batch_size, hidden_dim, src_len] \n",
    "        # and values as [batch_size, src_len, hidden_dim]\n",
    "        querries = self.q_layer(querries.transpose(0, 1))\n",
    "        keys, values = self.k_layer(keys), self.v_layer(keys)\n",
    "        keys = keys.permute(1, 2, 0); values = values.transpose(0, 1)\n",
    "        # attention should be [batch_size, 2^n-1, src_len], in log form.\n",
    "        # TODO add scale as option\n",
    "        log_attention = torch.matmul(querries, keys)\n",
    "        if(mask is not None):\n",
    "            # mask should be [src_len, batch_size] of bools to remove all padding in src. False is pad\n",
    "            mask = mask.transpose(0, 1).unsqueeze(1)\n",
    "            neginf_pad = (torch.ones(log_attention.shape) * -1e10).to(mask.device)\n",
    "            log_attention = torch.where(mask, log_attention, neginf_pad)\n",
    "        attention = nn.functional.softmax(log_attention, dim=2)\n",
    "        # print(attention.shape, values.shape)\n",
    "        # we want to matmul values and attention [batch_size, 2^n-1, src_len] * [batch_size, src_len, hidden_dim] -> [batch_size, 2^n-1, hidden_dim]\n",
    "        result = torch.matmul(attention, values)\n",
    "        # also transpose to conform with querries\n",
    "        return result.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class TemplateOutput(Attention):\n",
    "    def __init__(self, hidden_dim, template_num=128, dropout=0.2):\n",
    "        # disregard Attention's init (calling grandparent)\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # mimic the querries format\n",
    "        self.keys = torch.nn.Parameter(torch.randn(template_num, 1, hidden_dim).type(torch.FloatTensor))\n",
    "        self.values = torch.nn.Parameter(torch.randn(template_num, 1, hidden_dim * 2).type(torch.FloatTensor))\n",
    "        # override: load the keys regardless of the input value\n",
    "        self.k_layer = lambda x: self.keys\n",
    "        self.v_layer = lambda x: self.values\n",
    "        # q layers are made as normal\n",
    "        self.q_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def regularization_loss(self, reduce=None):\n",
    "        # extract losses using cosine distance between k & v. In effect, this cause the template to not repeat \n",
    "        # itself down to the lower level\n",
    "        keys = F.normalize(self.keys, dim=-1) # batch x 1 x dim\n",
    "        values = F.normalize(torch.squeeze(self.values, dim=1).reshape(keys.shape[0], 2, self.hidden_dim)) # batch x 2 x dim \n",
    "        transpose_values = torch.transpose(values, -2, -1)\n",
    "        loss = torch.matmul(keys, transpose_values)\n",
    "        if(reduce == \"mean\"):\n",
    "            loss = torch.reduce_mean(loss)\n",
    "        elif(reduce == \"sum\"):\n",
    "            loss = torch.reduce_sum(loss)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, inputs, num_apply_layer=3):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = inputs.view(batch_size, 1, self.hidden_dim)\n",
    "        outputs = [inputs]\n",
    "        for i in range(num_apply_layer):\n",
    "            # use the attention on the templates, loading a mock key value (None). We do not need mask, as every entry is equal\n",
    "            next_output = super(TemplateOutput, self).forward(outputs[-1], None)\n",
    "            # reshape the next_output to [batch_size, 2^i, hidden_dim], effectively split the values\n",
    "            next_output = next_output.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "            # dropout\n",
    "            next_output = self.dropout(next_output)\n",
    "            outputs.append(next_output)\n",
    "        # output in form of [batch_size, 2^n-1, hidden_dim], need to transpose to fit BucketIterator form\n",
    "        result = torch.cat(outputs, dim=1).transpose(0, 1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as torch_data\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Example, Field\n",
    "#torch_data = torchtext_data\n",
    "\n",
    "class ManualDataset(TranslationDataset):\n",
    "    def __init__(self, data, fields, **kwargs):\n",
    "        fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "        examples = [Example.fromlist([src_ex, trg_ex], fields) for src_ex, trg_ex in zip(*data)]\n",
    "        # use grandparent __init__\n",
    "        super(TranslationDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, data, hidden_dim, use_attention=True, dropout=0.2, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        self.use_attention = use_attention\n",
    "        fields = self.input_field, self.output_field = Field(lower=True, pad_token=\"<blank>\"), Field(lower=True, pad_token=\"<blank>\")\n",
    "        # write later\n",
    "        self._data = self.build_vocab(fields, data)\n",
    "        # encoder\n",
    "        self.encoder = Encoder(hidden_dim, hidden_dim, len(self.input_field.vocab), dropout=dropout)\n",
    "        # tree decoder\n",
    "        self.decoder = TemplateOutput(hidden_dim, dropout=dropout) #LayeredOutput(hidden_dim)\n",
    "        if(use_attention):\n",
    "            self.attention = Attention(hidden_dim)\n",
    "            projection_input = hidden_dim * 2\n",
    "        else:\n",
    "            projection_input = hidden_dim\n",
    "        # projection, adapt for both version\n",
    "        self.projection = nn.Linear(projection_input, len(self.output_field.vocab))\n",
    "    \n",
    "    def init(self, method=\"xavier\"):\n",
    "        # init all parameters using specific values\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, src, num_apply_layer=3):\n",
    "        #src = self.input_field(src)\n",
    "        encoded_out, encoder_seq = self.encoder(src)\n",
    "        output = self.decoder(encoded_out, num_apply_layer=num_apply_layer)\n",
    "        if(self.use_attention):\n",
    "            # create mask for src (values is NOT padding token). Shape is [src_len, batch_size]\n",
    "            pad_id = self.input_field.vocab.stoi[self.input_field.pad_token]\n",
    "            attention_mask = (src != pad_id).to(src.device)\n",
    "            # tokens = [[self.input_field.vocab.itos[i] for i in sample] for sample in src.transpose(0, 1)]\n",
    "            # raise ValueError(\"pad_token {}, \\n{}\\n{} - {}\".format(pad_token, tokens, attention_mask, attention_mask.shape))\n",
    "            # print(output.shape, encoder_seq.shape)\n",
    "            # apply attention and concatenate to the last dimension (hidden_dim)\n",
    "            attention_output = self.attention(output, encoder_seq, mask=attention_mask)\n",
    "            output = torch.cat((output, attention_output), dim=2)\n",
    "        projection = self.projection(output)\n",
    "        return projection\n",
    "    \n",
    "    def build_vocab(self, fields, data):\n",
    "        formatted_data = ManualDataset(data, fields) \n",
    "        # torch_data.Dataset([torch_data.Examples.fromlist([src_ex, tgt_ex], fields)\n",
    "        #        for src_ex, trg_ex in zip(*data)], fields)\n",
    "        [f.build_vocab(formatted_data) for f in fields]\n",
    "        return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> go <s> </s> </s> their <s> <blank> <blank> <blank> <blank> </s> </s> separate ways\r\n",
      "<s> <s> <s> Orlando Bloom and <s> </s> </s> </s> </s> </s> </s> Miranda Kerr\r\n",
      "<s> <s> <s> each other Actors <s> </s> </s> </s> </s> </s> </s> Orlando Bloom\r\n",
      "<s> and <s> </s> </s> Model <s> <blank> <blank> <blank> <blank> </s> </s> Miranda Kerr\r\n",
      "<s> still <s> </s> </s> love <s> <blank> <blank> <blank> <blank> </s> </s> each other\r\n",
      "<s> , <s> </s> </s> in <s> <blank> <blank> <blank> <blank> </s> </s> an interview\r\n",
      "<s> parents <s> </s> </s> to <s> <blank> <blank> <blank> <blank> </s> </s> two-year-old Flynn\r\n",
      "<s> <s> <s> Miranda Kerr and <s> </s> </s> </s> </s> </s> </s> Orlando Bloom\r\n",
      "<s> , <s> </s> </s> supermodel <s> <blank> <blank> <blank> <blank> </s> </s> Miranda Kerr\r\n",
      "<s> <s> t life <s> </s> </s> </s> </s> doesn ' <blank> <blank> <blank> <blank>\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 /workspace/khoai23/example_tree_ende.lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "#sourcefile = \"\"\n",
    "#targetfile = \"\"\n",
    "# treefile = \"/workspace/khoai23/example_tree_ende.lines.txt\"\n",
    "def loadTree(location):\n",
    "    with io.open(location, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [l.strip().split() for l in f.readlines()]\n",
    "treefile = \"/workspace/khoai23/train_tree_ende.lines.txt\"\n",
    "#with io.open(treefile, \"r\", encoding=\"utf-8\") as f:\n",
    "#    targets = [l.strip().split() for l in f.readlines()]\n",
    "targets = loadTree(treefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the source phrase by correct tree position and ommiting keys\n",
    "positions = [7, 3, 8, 1, 9, 4, 10, 0, 11, 5, 12, 2, 13, 6, 14]\n",
    "def create_sources(tokens, reorder_positions, omissions=[\"<s>\", \"</s>\", \"<blank>\"]):\n",
    "    regtok = [tokens[i] for i in reorder_positions]\n",
    "    return [tok for tok in regtok if tok not in omissions]\n",
    "# apply\n",
    "sources = [create_sources(ts, positions) for ts in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the target/source pairing for 3 & 5 as well\n",
    "def create_positions(depth):\n",
    "    if(depth == 1): \n",
    "        return [0]\n",
    "    prev_depth = create_positions(depth-1)\n",
    "    # next_depth = range(2 ** (depth-1) - 1, 2 ** depth - 1)\n",
    "    # interleave: next_depth will surround its parent node in prev_depth. Actually don't need the next depth either\n",
    "    joined = []\n",
    "    for i in prev_depth:\n",
    "        if(i < 2 ** (depth-2) - 1):\n",
    "            # belong to too-far-back depth, ignore\n",
    "            joined.append(i)\n",
    "        else:\n",
    "            # intermediary node, append children\n",
    "            joined.extend([i * 2 + 1, i, i * 2 + 2])\n",
    "    return joined\n",
    "test_pos_4 = create_positions(4)\n",
    "assert all([x == y for x, y in zip(test_pos_4, positions)]), \"{} - {}\".format(test_pos_4, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_3 = loadTree(\"/workspace/khoai23/train_tree_ende_3.lines.txt\")\n",
    "positions_3 = create_positions(3)\n",
    "sources_3 = [create_sources(ts, positions_3) for ts in targets_3]\n",
    "\n",
    "# 5 is not working\n",
    "targets_5 = loadTree(\"/workspace/khoai23/train_tree_ende_5.lines.txt\")\n",
    "positions_5 = create_positions(5)\n",
    "sources_5 = [create_sources(ts, positions_5) for ts in targets_5]\n",
    "\n",
    "sources_4, positions_4, targets_4 = sources, positions, targets\n",
    "sources = sources_3 + sources_4 + sources_5\n",
    "targets = targets_3 + targets_4 + targets_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Parliament', \"'s\", 'legislative', 'prerogative'], ['<s>', '<s>', '<s>', 'Parliament', \"'s\", 'legislative', 'prerogative']), (['purpose', 'and', 'extent'], ['<s>', 'purpose', '<s>', '</s>', '</s>', 'and', 'extent']), (['the', 'national', 'bureaucracies'], ['<s>', 'the', '<s>', '</s>', '</s>', 'national', 'bureaucracies']), (['null', 'and', 'void'], ['<s>', 'null', '<s>', '</s>', '</s>', 'and', 'void']), (['down', 'in', 'advance'], ['<s>', 'down', '<s>', '</s>', '</s>', 'in', 'advance']), (['Stretching', 'and', 'Pilates'], ['<s>', 'Stretching', '<s>', '</s>', '</s>', 'and', 'Pilates']), (['and', 'Postural', 'Gym'], ['<s>', 'and', '<s>', '</s>', '</s>', 'Postural', 'Gym']), (['personal', 'Trainer', 'and', 'Instructor'], ['<s>', '<s>', '<s>', 'personal', 'Trainer', 'and', 'Instructor']), (['Stretching', ',', 'Pilates'], ['<s>', 'Stretching', '<s>', '</s>', '</s>', ',', 'Pilates']), (['the', 'Italian', 'Federation'], ['<s>', 'the', '<s>', '</s>', '</s>', 'Italian', 'Federation'])]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(sources[:10], targets[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    With label smoothing,\n",
    "    KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "    and p_{prob. computed by model}(w) is minimized.\n",
    "    \n",
    "    Pulled from OpenNMT; adding special smoothing for specific tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, label_smoothing, tgt_vocab_size, ignore_index=-100, special_smoothing=None, special_indices=None):\n",
    "        assert 0.0 < label_smoothing <= 1.0\n",
    "        self.ignore_index = ignore_index\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "\n",
    "        smoothing_value = label_smoothing / (tgt_vocab_size - 2)\n",
    "        one_hot = torch.full((tgt_vocab_size,), smoothing_value)\n",
    "        one_hot[self.ignore_index] = 0\n",
    "        self.register_buffer('one_hot', one_hot.unsqueeze(0))\n",
    "        self.confidence = 1.0 - label_smoothing\n",
    "        \n",
    "        if(special_smoothing is not None):\n",
    "            one_hot_s = torch.full((tgt_vocab_size,), special_smoothing / (tgt_vocab_size - 2))\n",
    "            self.register_buffer('one_hot_s', one_hot_s.unsqueeze(0))\n",
    "            self.confidence_s = 1.0 - special_smoothing\n",
    "            def indices_check(target):\n",
    "                is_special = target == special_indices[0]\n",
    "                for i in special_indices[1:]:\n",
    "                    is_special = torch.logical_or(is_special, target == i)\n",
    "                return is_special\n",
    "            self.indices_s = indices_check\n",
    "        else:\n",
    "            self.one_hot_s = None\n",
    "            \n",
    "        self.register_buffer('zeros', torch.zeros((tgt_vocab_size, )))\n",
    "\n",
    "    def forward(self, output, target, reduction='batchmean'):\n",
    "        \"\"\"\n",
    "        output (FloatTensor): batch_size x dim x n_classes\n",
    "        target (LongTensor): batch_size x dim\n",
    "        \"\"\"\n",
    "        # flatten both\n",
    "        old_shape = output.shape\n",
    "        target = target.reshape(-1)\n",
    "        output = output.reshape(target.size(0), -1)\n",
    "        # also output need to be log softmaxed\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        model_prob = self.one_hot.repeat(target.size(0), 1)\n",
    "        model_prob.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        # create and join the probs together\n",
    "        if(self.one_hot_s is not None):\n",
    "            s_prob = self.one_hot_s.repeat(target.size(0), 1)\n",
    "            s_prob.scatter_(1, target.unsqueeze(1), self.confidence_s)\n",
    "            # join: if true = special = use s_prob\n",
    "            s_selector = self.indices_s(target)\n",
    "            # raise ValueError(s_selector.shape, s_prob.shape, model_prob.shape)\n",
    "            model_prob = torch.where(s_selector.unsqueeze(1), s_prob, model_prob)\n",
    "        model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)\n",
    "        output = torch.where((target == self.ignore_index).unsqueeze(1), self.zeros.unsqueeze(0), output)\n",
    "        # rescale input before feeding to kl_div (same as cross entropy, save a constant value) to allow correct calculation of batchmean\n",
    "        output = output.reshape(old_shape)\n",
    "        model_prob = model_prob.reshape(old_shape)\n",
    "        return F.kl_div(output, model_prob, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 3 2\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "data = sources, targets\n",
    "model = Model(data, 256)\n",
    "\n",
    "blank_id = model.output_field.vocab.stoi['<blank>']\n",
    "pad_id = model.output_field.vocab.stoi['<pad>']\n",
    "unk_id = model.output_field.vocab.stoi['<unk>']\n",
    "inner_id = model.output_field.vocab.stoi['<s>']\n",
    "outer_id = model.output_field.vocab.stoi['</s>']\n",
    "print(blank_id, pad_id, unk_id, inner_id, outer_id)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=blank_id)\n",
    "#criterion = LabelSmoothingLoss(0.2, len(model.output_field.vocab), ignore_index=blank_id,\n",
    "#                              special_smoothing=0.5, special_indices=[inner_id])\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "graphview = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "\n",
    "graphview = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd4VFXixvHvSScUAxIsFENXXBUkKoqAgAjCT3Hdta5dF3XV1RXdxbV3rKtrWcXeexfpoCjSQu8QIPQSSiCUhJTz+2Mmk5lkkpkkM5k7yft5Hh5n7tx758w1eXPm3FOMtRYREYkeMZEugIiIVI2CW0Qkyii4RUSijIJbRCTKKLhFRKKMgltEJMoouEVEooyCW0Qkyii4RUSiTFw4Ttq8eXOblpYWjlOLiNRJc+bM2WGtTQ1m37AEd1paGhkZGeE4tYhInWSMWRfsvmoqERGJMgpuEZEoo+AWEYkyCm4RkSij4BYRiTIKbhGRKKPgFhGJMo4K7pcmreKXldmRLoaIiKM5Krhf/Xk10zJ3RLoYIiKO5qjgBtDixSIilXNUcBsT6RKIiDifo4IbQBVuEZHKOSq4VeEWEQnMUcENoAq3iEjlHBXcRo3cIiIBOSq4QW3cIiKBOCq4Vd8WEQnMUcEtIiKBBRXcxph/GGOWGGMWG2M+McYkhatAVrcnRUQqFTC4jTEtgb8D6dbaPwCxwKVhKY3aSkREAgq2qSQOaGCMiQOSgc3hKpBuToqIVC5gcFtrNwHPAuuBLcAea+34cBRGFW4RkcCCaSppCgwF2gJHAw2NMVf42W+YMSbDGJORna2pWUVEwiWYppKzgbXW2mxrbQHwNXBG2Z2staOstenW2vTU1NRqFUYDcEREAgsmuNcDPYwxycaVrP2BZeEqkKZ1FRGpXDBt3DOBL4G5wCL3MaPCURhVuEVEAosLZidr7YPAg2Eui+u9auNNRESimKNGTqrCLSISmKOCG9SPW0QkEEcFt3qViIgE5qjgBs1VIiISiKOCW/VtEZHAHBXcoDZuEZFAHBXcauIWEQnMUcEN6sctIhKIw4JbVW4RkUAcFtwiIhKI44JbNydFRCrnqODWzUkRkcAcFdwuqnKLiFTGUcGtCreISGCOCm5QG7eISCCOCm61cYuIBOao4AbVuEVEAnFUcBu1couIBOSo4AZN6yoiEoijgltt3CIigTkquEFt3CIigTgquFXhFhEJzFHBDRo3KSISiKOCW4sFi4gE5qjgBrVxi4gE4rjgFhGRyim4RUSijOOCWwNwREQq56jg1r1JEZHAHBXcgPoDiogE4KjgVo1bRCQwRwU3qMItIhKIo4Jb07qKiATmqOAGsBqBIyJSKUcFt9q4RUQCc1Rwg9q4RUQCcVRwq8ItIhKYo4IbNMmUiEggjgpuTesqIhJYUMFtjEkxxnxpjFlujFlmjDk9XAVShVtEpHJxQe73IjDWWvtnY0wCkByOwqi+LSISWMDgNsYcBvQGrgGw1h4CDoWrQOrHLSJSuWCaStoC2cA7xph5xpg3jTENw1IaVblFRAIKJrjjgJOB/1lruwH7gRFldzLGDDPGZBhjMrKzs6tdINW3RUQqF0xwbwQ2Wmtnup9/iSvIfVhrR1lr06216ampqdUqjCrcIiKBBQxua+1WYIMxprN7U39gaTgKs31vPrv2ha35XESkTgi2V8ltwEfuHiVrgGvDUZjc/EKmr9kZjlOLiNQZQQW3tXY+kB7msoiISBAcNXJSREQCU3CLiEQZBbeISJRRcIuIRBkFt4hIlFFwi4hEGQW3iEiUUXCLiEQZBbeISJRRcIuIRBkFt4hIlFFwi4hEGUcG9/78wkgXQUTEsRwZ3GMWb6WoWGvhiIj448jgvuuLBbw4cWWkiyEi4kiODG6ARZv2RLoIIiKO5NjgFhER/xwb3MZo6WAREX8cG9wiIuKfY4N7+Za9kS6CiIgjOTa4N+/Jq7X3WrUtl+fGr8BadUEUEedzVHCnNk70eZ42YjQPfb8k7O976agZvDQ5k70HNfBHRJzPUcH93S09y2179/csNu4+wKRl28L2vgVFxWE7t4hIqMVFugDejk5p4Hf7mU9NAWDpIwNJTnBUkUVEap2jatyBhHsUvEVt3CLifFEV3OGiPuMiEk2iLrjzC4so1gRUIlKPRVVw788vpPN9Y3n4h/D3NBERcaqoCu7TnpgEwHvT14Xl/OrGLSLRIKqC29ua7H3MXLOTwqJiPpieRaG69IlIPeG4vnUN4mM5WFAUcL9+z/0CQLc2Kcxbn0N+YTFXnn4MX2Rs5PJT2xATU/UbjrpHKSLRwHE17pn39mf4gE5B7z9vfQ4Aj41exqmPT+K+bxfz/YLNANzwXgYfzHA1q1hr2ZxzMPQFFhGpZY4L7iZJ8bQ5PLlax+45WAC4mlEAJi7bxv3fLgbggxnrOGPkZJZsrniBBrVxi0g0cFxwA3Q/pmmNjv/v5Mxy22au3QXA6uz95V5TE4mIRBNHBnerptWrcXsrqX2XiHWns/qAi0i0c2Rwh8JJD4/3PM4vLKLkXmWxteQXFjFlxXbemLrGZypXRbqIRAPH9SopceeATjw/ITQrvd/1xUJ+cN+wLLbw4HdL+HT2BgB6dWrOwUOBe7GIiDiFY2vcf+/fkXn3DwjJuUpCG+CuLxZ4Qhtg1C9ryC909QFfuDEnJO8nIhJOjg1ugKYNE8L+Hl/P2+R5fMBPzXvDrgMa3CMijhJ0cBtjYo0x84wxP4azQGVd2eOY2nw7H9v35tHr6Sk88dPyiJVBRKSsqtS4bweWhasgFbnklNa19l5l+3Hv3H8IgJ9XbldvFBFxjKCC2xjTChgCvBne4pRXVIuBmbXT1cfbWusT1Guy93P9e7OZvyGHDbsO1Fp5RET8CbZXyQvAP4HGYSyLX+1bNKq193pm3AqeGbfC8/yda07xPJ6yIpspK7IBGHVld54Zt4Ixt/ciLtbRtwlEpA4KmDrGmP8Dtltr5wTYb5gxJsMYk5GdnR2yAjZKjCNr5BAy7js7ZOcM1rXvzva7fdgHc1i1fR+5ea5V4dfu2O8ZZi8iEm7BVBd7AucbY7KAT4F+xpgPy+5krR1lrU231qanpqaGuJgQH+O8mu0178wir6CIvs/+7JmtUEQk3AKmobX2HmttK2ttGnApMNlae0XYS1bGYcnxvH5l99p+20ot2LiHv3001+9r1lpe/TmTHfvya7lUIlLXOa8aW4mBxx9J1sghJMU7p9iTl2/3PB67eCvrdu5n65485m3I4emxKxj++YIIlk5E6iJjwzCXaXp6us3IyAj5eUvkFRSxbucBBr4wNWzvUVNHHZbElj15AHx7S08ueX06k+86i5YpDSJcMhFxImPMHGttejD7OqfqWgVJ8bF0PrIxs+7tH+miVKgktAEueGUa+YXFjF+yNYIlEpG6IiqDu0SLxkkA3D2wM5OG94lwaQIr6Rr+0PdL+DxjQ+U7i4hUwLGzAwYra+SQSBchaCXNUu/+ngXAxem1NypUROqOqK5xl/XOtadwW78OXH165OY3qcxjo5eVW+BBRKSqor7G7a1v5xb07dwCgB8WbqGgqJi3rzmFi16bHuGSlfJe4EFEpDrqVHB7m/Vv143LuNgYPrz+NK54a2aES1TewUNFNEiIjXQxRCTK1KmmEm9xsTGeeUROa9cswqXxLzs3n3B0xxSRuq3OBre3+NgYXry0a6SLUU7vZ6bQ9p6fyMjaFemiiEgUqRfBDXDeiUdzW78O9OrYnIu6t2Ltk4N56LwukS4WAL+u2hHpIohIFKmzbdxlxcQYhp/T2WfbH09uxUM/LI1QiUqpsUREqqLe1Lj9OaxBfKSL4KJ2bhGpgnod3ADvXXcqN/ZuF9EybNh9kOfGr2BffmFEyyEi0aHeNJVUpE+nVPp0SiVr537GLdkWkTJ8415pfl9+IQ+ed3xEyiAi0aPeB3eJ169MJ7+wCIDO942NSBnyCooj8r4iEl3qfVOJt8S4WBLjYmkQ7xoUs/zRQVE1F4qI1A8Kbj++uvkMbu3bgcS42r08+/IL2bEv32eF+fpi6548pq/eGeliiEQFNZX40eXoJnQ5ukm57VPuOotDhcUs3rSH4V/4rmyTEBfDocKaNXX8sGAzPyzYzN/7d+TOAZ1Yu2M/Bkhr3rBG540Gg16cSs6BAn3DEQmCatxV0LZ5Qzof2Zg/dW9V7rX5DwwI2ftMXu66Sdr32Z8569mfQ3ZeJ8s5oFkTRYKl4K6mnh0OB2D2vWcz/4EBJCfEcVWIppNdv/OA3zlM8gqK2Oq1sk5N5eYV8MH0LM2XIhJlFNxBuOucTvy9f0efbW9dfQqz/t2f1MaJpCQnAPDw+ceT+fi5NX6/vXmFvPrz6nLbb/5wDj2enFRu+4cz1rFr/6Eqv8+D3y3h/u+WMGON5koRiSYK7iDc2s/V5uwtKT6WFk2SfLYZYzwzEjZJKr190KMasxO+M22t5/HLk1cBMGVFdrn9VmzN5b5vF3P7p/Oq/B6/ZbrmSHnip2WqdYtEEQV3GHx8w2mM+0dvz/NPh51e5XPs2Fdag352/MoK9yu5Ibr7QNVr3Ntz8wFYtGkPy7bkVvl4EYkMBXcYnNGhOUcd1gCAoV2PBvC0fzdvlFitc/Z6enJoClcBq6muRKKGugOGkXfXtgfPO55/DTqWs5//pVrn2rDrYKiKJSJRTjXuWhIbY2iYGPq/k6GqKRtMSM4jIuGn4K5lL19+cljOW9PgHfLSr8zfkENeQVGISiQi4aLgrmXdj2nKqhp2Gdyw6wAAizbu4fyXp4WiWFgLF7wyjXu/WUxhUXG1uheKSO1QG3cExMfGkDVyCGkjRlfr+F5PTwHwTIYVyH8nraJRYhzXndk24L6LN+3hkR+X8v70dSx5eGBYmndEpGZU445iB8s0a1hr2bLHdROzsKjY01Xw+QkreeTH4JdoG71wCwAHDqnZRMSJVJ2KoNgYQ1GIZgLcuS+ftvf8BMCk4X0476XfOHCoqEaTNpkAzeYFRcV0vHcMAGufHIwJdICIhIRq3BG06rGaD48vsdlrDpNpmTs8teV563d7to9dvJUxi7ZUeh5jShcv3pKTx859+Z7X9ucXcuGr01i1zTVY58s5Gz2v1cOZaEUiRsEdQeGqoD7w3RLP46krd3ge3/ThHG7+aC73fbsoqJr+eS//RvfHJnqe/5a5g7nrcxjwn6nkFRSR79VUU6wh8yK1RsEdQbXRtPCfieWHy384Yz2fzl7vd/9DfnqU+JvHZMuePJ/y+/tDsGpbLn94cBybczR4SCSUFNz1lHczh7c12fvLbXvtlzXljvl5xXaWby2d3yQ7N7/ccR/NXM++/ELGLt5a0+KKiBcFdz01b31O0PuOWbyF056YyISl2zzbHv5hKZ/MKq21ew/ln756J3PX7/bU1B/5cSm/rCw/s2GwrLV8NHMdew66Flu48YMM3vs9y/N6QVExBw4VVvv8ItFGwS0BLdy4h217y9eoveUXFnuC9bI3ZnDhq79T6NV88sgPSzh4qIhXpmRSWFS1Jd7mb8jh3m8Wc8/XCwEYt2QbD35f2o5//XsZdHlgXJXOKRLNFNwSMic9PJ67vNbi/GhmaY08v7CY2z6ZxzPjVvDxLP/t6xXJd/dH35Hr2/aeud3VVDPVqzb/4Yx1pI0Yzd686i+FZq3loPqwi4MpuCWkKmo737j7IBOXuZpavHu9BKPkFmjZCbXOfn4qV709y2fbu+4mlJos8fbKlEyOe2Cshv2LYym4xVGKii1z1+9mhdeNz5LeK7OzdpNf6FsT9q5tFxVbzyRZO/cd4rEfl3LgUCFP/LSM3CrUwL+bvxnwf8NVxAkCjpw0xrQG3geOwDU2Y5S19sVwF6y++e1ffTnzqSmRLkatOfb+MeQVuJpA/jXoWM/2aZk7PLXoD64/lVPSmnHx69M9r//iZ/m2Eu3//ZPn8SM/LmXZlr28+ZtrCbjCIstlp7Zm1fZ9DD7hqErLpgGg4nTBDHkvBIZba+caYxoDc4wxE6y1wU9+IQG1apoc6SLUqpLQBnhq7HLPY++mjyvf8m0GAYKefXzZlr0+z7+bv4m33et4BjsNgFYFEqcK2FRird1irZ3rfpwLLANahrtg9dHXfzsj0kVwvBs/mFOt43ZWob26sMgV2E4ZDPre71ms3VG+f73UX1Vq4zbGpAHdgJnhKEx9d3KbppEuQr2walsus9buqvD1Ne6QnLJiO+DqZZJXUMRtn8yr9cFEhwqLefD7JfR99mfdLBWPoIPbGNMI+Aq4w1q718/rw4wxGcaYjOzs6g+2qI/iY0sbVVumNIhgSeqHAf+Z6tNuXpGcA64bmm/9tpZj7x/LDws2c9OH1avxV5f3HDAnPzrBM6gpN6+A8Us0IrW+Ciq4jTHxuEL7I2vt1/72sdaOstamW2vTU1NTQ1nGOu2pP53AmNt7e56P/0dv5tx3NuPu6F3JURJKBRUMCCoJye8XbK70+E05B9meW/3uh5XJ2unbRDJpmetbwD8+m8+wD+awfueBsLyvOFswvUoM8BawzFr7fPiLVL9cckobn+cNE+NomBhH9j51RasNYxdv4aYP5zL2jl4ce2QTn9fe+HUtSfGxLNy4p9xxSzfvJSU5nqNTGtBz5GQg+JueVbFwg+97vzBpJQs25rBy2z6g/GIa0Wbu+t3MXbebG3q1i3RRokowNe6ewJVAP2PMfPe/wWEuV73Xpln96mUSKT+6V/v5ccEW0kaMJiPLt+37pcmZ5Y75bdUOBv/3V84YObnC2nq4LN60l5cmZ7Leve5oSc+XgqJiznxqctRN6HXhq7/z2OhlkS5G1AmmV8lv1lpjrT3RWtvV/e+nQMdJzSQnxHFL3/aRLkaddumo6Z7gfnmKK6Dv+XpRwOOueKv03vy935Tuv2zLXtJGjGbVtlxWbsslN6+A7XvzuPi16T4LUlRFoHnOi4shr6CIjveOYePug9z37eJqvU+kBXPPQUpp6TIHu61fRxLjYnl+Qvk5taXmZqwp37Nk1fZ9VTrH5xmlQ/zPffFXAIa+Ms2zAlGvjs2ZlbWLe75exOtXdveZw7y42FJsLXGx5etPu909SN71mgXRnxcmruSh84/3PC8oKmbZlr0cd1STSo5ynsp6+TjdbZ/MIz7W8PzFXWvtPTXk3cGS4mO5oVfpyuxnddZN32jgvcjyr6tcKxCNX7qNro9MIG3EaM8izle8NZMO7jU7y+r26AS6PTrBZ85zf8Yv3eaZlRFgz8ECzn3xV9JGjObZcStq+lGC8uava3w+V7Be+2V1mEpUu35YsJmv526q1fdUcDtcjLuGlhgXQ6NEfUGKZiUBWzJz4e+rd/rdr6rt5ue//Jvf7S9PySSvoIjMCr5FXPzadN6YuqZK71XR+4BrTdKqGDlmeeCdHGbEVwtJGzE60sVQcDtdSXA3b5TIsN7tFN51xAWvTPN5/u28TaSNGM2oqavpWEEtvCIFRRW3g9/5+XzOfv4Xv6E6K2sXj//ke2Pw9V9Ws3Sz7zCN0Qu3eCbvqsyCjTn8vnpHwP1C5fOMDfy6KnRjRr6as5G0EaMrnVny09kbANecOpGk4Ha4hLgYnr/4JD6/6XRObJXC4ocHcv//dYl0saQGrnt3NvM3lK5AlDZiNHd8Nh+AJ34KbS20pFZ/qLCY3LyCShexsNby5JjlDP7vr55tl78xg1s+nsvDP1Q8NVFJq/0178zm8jdqb1D1P79c6Hc+m+r6aq7rfkVFf3yOu3+s5/Ff3ozs4HEFdxS48ORWPiMqG8THAvCnk1uF5PyvXdE9JOeR4PjrFx4uJaM/AU54aDzXvjubOz+b77PsHLhC21/TTcm2sYu3cMKD4xj2fgYTl27jlo/mevapjUWvA5m7fjc7ajj2oWTBjjs/X+D39bJ95tNGjOb+CPXi0ffuKOY9VL4mzulyREjOI85XcrP063mlN9P25hUwdvFW/vnlQs+23zN3sDevtHllt/sPwPil2xjvXnv0tOlZbNmTF7I5VErajtunNmTS8LM829dk7+OYwxsSG+P/533QC1NZvjWXlikNmDaiX7Xe21rLnHW7fcoyeXgfxizeyjPjVjDi3GP9HvfBjHWex/M35NC1dUq13r+qVOOOQv4qOJ/8tUe1zxcTY1j+6KAalEic7sVJqyp87cSHxvs03QBc/ubMgPOyPPDdEv73c/meIXPW7eK0JyYyrppzqazO3o+1lu8XbOb2T+fR77lfeKaSHjIlPW825Rzkge+qVwNe42f2xevfy/C8bzA3Usvetwgn1bij0JkdmgNwUXprrjz9GJLiY2mf2ojljw7i+wWbfWpOwUpyN78AfHdLT4bW4g+hhF+g/uAfz6zaOqCV+dP/XINpbvxgDlkjh5C5PZeV2/aREBvDBzPWkV9YxMc3VF7ReH/6Op8Fob+cs5EmDeI4kF/EXQM7V9iD5f3p67g4vTV/aHkY4BoUtWNfPr06Vt6V1l/XSeuUeX39UHBHodbNkv3Oi5EUH+tp/66JBgk1P4cIUGHXuVs/met3ewnv0AbYsS+fp8e6wrX7MU259t3ZFR77+tQ1vHRZN6B0UFSgeWTKfuMAyKrGBF7v/Z7F1WekVfm4qlJTSR1TUtMoa979A/j4r6cFdY6UBvF8e0tPJt7Zm89vPL3SfUf//UxWPnYuQwIsBybi7adF1Z9TpbLQBteAGO+250AWbdzDlhosLu2t7B+ccFFw1zFtmzcka+QQLju1tc/2pg0TOKN980qPjXPf/GnRJImurVPo0KIxbZs3LLffxDtLp5xNjIshIS6G9DQtAiHOcf+3ixmzaIvn+S8rs/k8YwO79x9i6CvT2Li7tDZ9XgUDmJxMwV1HeTfPXX9m2wr3635MaeCO+0dvRl54gs/rqY0Tfb5m3j2wMx1aNOaNq9I5vGEC7VMbAQRcgPfFS7uSNXIIKx87tyofQ6TaSlYwArj67Vn888uFdHt0Ags25DDohV9ZvrXcejBRQ23cddTdAztTWGx57II/+Nx4HD6gE895TVr15U2lTSHtUxt5grisy05tzYAuR9DvWFfXwQFdjmBAlwGe1xPjXHWA3p1SWbxpD7v2H2JY73aMcg+pHtrVtUxpQpzqClI7Kmv+2JdfyKAXfmVAlHaFVXDXUYc3SuTZi04qt/2Wvh0476Sjmb8hh49nBd+T4MkLT6z09ZTkBD4b1oMuRzehcVK8Z/uoEMyFIVIdJX3WKzPB3Sc92ii465mYGENa84akNW/IBd1ahvTcp7U7PKTnExH/9L1Vwu6aMt2jvrq58p4qFQnVSFGRaKfglrDKGjnEZ6J/gO7HNOOfgzpX+VxLHxnEsUc25p1rTwlV8USikoJbIuKcLkd6Hr9bSRCnJLvay5/44wnEx8Yw9o7e9O3cgtVPDPaZeKtF40TP40TdAJU6Tm3cEhEdWjSqdDTbFT3a0LppMjf28b/uZmyMYdqIfp6ReRP+0YdNOQe54NVpPP3nEzk6pQEPfLeEuwd2Yt76HF6anMmdAzppGTipExTc4gjzHxiAwfDmb2t4aXImj11wQuCDvByWHM9hyfE+/cTH3N4LgLnryg9nrq5pI/rRc+TkkJ1PpDr0nVIcISU5gcOS4xl+TueA80pUlcU1GqmiW5vxsYbJw/sweXgfn1kSG5dZbejCbi19mmf8GXWl5jaX8FNwS513cXprUpLjuaBbS77+2xme7SUh/O0tPWmX2oh2qY18BiuVvQn6/CWuVbxn3dufy05tU+59hvVuxznHH8maJwaH42OU650j9ZeCW6Lalzed7jN3ij/HHN6Q+Q+cQ+tmyZzcpnSIf+Mk/y2FbZs35J5zjyU9rRlZI4dwcXorOrYoHVHaonES551Ufoj/vwcfB7j6yofa1Lv7ckpas5CfV6KT2rglqqVXI8z+dlZ7jjwsiR8XuCYhiovxrb9Muessn+dP/7n8CFTjbng5tW0zLureijQ/k3GBq1a/KedglctYVpvDkz3T7Q7tejQDuhzBrR/Pq/F5JTopuKXe+ecg1zJUg44/km/mbaLTEf7nZ6lMUrwr7FMaxHNReusK9/vmljPI3L6vWovo/nL3WfR55mfP87ITfvkL7nO6HOFZWixatG7WgA27av7HrTa8eGlXNufk8dTY0C7qXFVqKpF6q0WTJG7s075ai912bZ3Cw+cfz9N/rnwOlxaNkzijfXOm3t2XWff2p3FiHNf1bMt9Q44L+B5tmiUHVZYBXY5g6t19aZwYx78HH0fm49E1A+NhDeID71TL5t4/wO86k0O7tuQvPVz3N9KPidxUxqpxi1SDMabSlU6m3t2X3PzSFdbbHO4K4UUPD/RsO/+ko7nuvdks3uSaXvSvvdryxq9rAbjn3GMxxtC3cyr7831XFy/rjavSy507mjhthbCSbzU39WlPdm4+b/22licvPIFz/+AaNNYkKZ5Vj59LXIzhzKemkNY8mY9u6FHhaj/hoOAWCYOSoK5MiyZJ/HDrmSzatIcTW6VwqLCYMzo05+ChIgYe7wqJd649tcLjJ97Zh5wD/ldY//aWnj6L135+4+lc/Pp0Ygyc2Cql3FJdN5/Vnl37DlFQXMzXczeVPZ2P2/t3rHTx4aoq9gruT4f14NJRM0J27pq665zOtGmWzCXprX1uOsfHuhorvFeVT06I5cChyv/IhoqaSkQiyBjDia1SANdc5X07t2DwCUcRG0TPlA4tGlV4c7Zdqu/N0qbuqQMs8Mlfe9C8kWuKgFf/cjLzHxjAvwYdy1N/PpHnL3YtePF/J7p6zcy6tz/feHWhBMqtduQ96OmOszsGLHdZ3tMVnNbW9/PM/Hf/Kp+vJsou9NEgIZarz0gLqqdQxn1ns6SWvvUouEXqoCZJ8WSNHMK7155Cy5QGtHa3l196ShsaJMTSqqmrD/sRTZJISU4od/x/LunKz3edRYvGSXRr05QnvVZGahAf61PTTIiL8fRrL/mDAK4gC8aLl3Zl4p2uAVBl7zcc0SSJZ9z3EZ74Y9VG01ZHTRb6SE6Io2Fi7TRiKLhF6rCzOrdg2oh+JMXHsvzRQTx+wR8AeOmybvzltDZ0bZ3i97j42BifLo6XndqGbm1c+xpjaJnSgITYGE9t+cbe7eh0RCOfJeyd7YwSAAAGnElEQVS8Qxzgs2E9SIqP4ZGhx7P2ycG8fU06n/y1BynJCXRo4RoABXBR91YA9O2c6nqe3pqskUO4/LQ2fOFesemRob4zTgKe1Wyu7ZlWtYsUhYwNw52B9PR0m5GREfLzikjkjPhqIZ/O3sCk4X1on9qIgqJiDBAX61v/y9yey5LNexnatSUbdh0gJsYEnCrA2/qdB/j7p/N479pTOSy54h4nZW8GXtezLW9PW8t9Q47jhl7teGVKJs+MW1Glzxjq6Raqwhgzx1qbHsy+qnGLSFAeOv94PhvWw7MuaXxsTLnQBujQorFnjdHWzZKrFNrgurH77S09Kw1tcK2f+umwHqQ2TvT7+t/OKp1ZcvAJrpu9jRLjuMRPv/tHhx7P/AcGlNvuVOpVIiJBSYqPddTydLf1d90InTy8DwcPFfHhjHUAnhGm3u3lXY5qwp0DOtOsYQIFRcUs35bLm1elc8rjE7kkvTVXnp5W6+WvCTWViEidkFdQxDvTsvhrr7aebwKz1u7i5g/n8Nu/+nkC3dvBQ0UkxMUE1Ysn3KrSVKLgFhFxALVxi4jUYQpuEZEoE1RwG2MGGWNWGGMyjTEjwl0oERGpWMDgNsbEAq8A5wJdgMuMMV3CXTAREfEvmBr3qUCmtXaNtfYQ8CkwNLzFEhGRigQT3C2BDV7PN7q3iYhIBITs5qQxZpgxJsMYk5GdnR2q04qISBnBBPcmwHuMaCv3Nh/W2lHW2nRrbXpqamqoyiciImUEHIBjjIkDVgL9cQX2bOBya+2SSo7JBtZVs0zNgR3VPLYu0XVw0XVw0XVwqcvX4RhrbVC13oBzlVhrC40xtwLjgFjg7cpC231MtavcxpiMYEcP1WW6Di66Di66Di66Di5BTTJlrf0J+CnMZRERkSBo5KSISJRxYnCPinQBHELXwUXXwUXXwUXXgTDNDigiIuHjxBq3iIhUwjHBXR8msjLGZBljFhlj5htjMtzbmhljJhhjVrn/29S93Rhj/uu+HguNMSd7nedq9/6rjDFXR+rzVIUx5m1jzHZjzGKvbSH77MaY7u5rm+k+NvIz4/tRwXV4yBizyf1zMd8YM9jrtXvcn2mFMWag13a/vy/GmLbGmJnu7Z8ZY8ov4e4AxpjWxpgpxpilxpglxpjb3dvr3c9EtVhrI/4PVzfD1UA7IAFYAHSJdLnC8DmzgOZltj0NjHA/HgE85X48GBgDGKAHMNO9vRmwxv3fpu7HTSP92YL47L2Bk4HF4fjswCz3vsZ97LmR/sxVuA4PAXf52beL+3chEWjr/h2Jrez3BfgcuNT9+DXg5kh/5gquw1HAye7HjXGNFelSH38mqvPPKTXu+jyR1VDgPffj94ALvLa/b11mACnGmKOAgcAEa+0ua+1uYAIwqLYLXVXW2qnArjKbQ/LZ3a81sdbOsK7f2Pe9zuUoFVyHigwFPrXW5ltr1wKZuH5X/P6+uGuU/YAv3cd7X1NHsdZusdbOdT/OBZbhmgOp3v1MVIdTgru+TGRlgfHGmDnGmGHubUdYa7e4H28FjnA/ruia1KVrFarP3tL9uOz2aHKruwng7ZLmAap+HQ4Hcqy1hWW2O5oxJg3oBsxEPxNBcUpw1xdnWmtPxjW3+S3GmN7eL7prBvWym099/uzA/4D2QFdgC/BcZItTe4wxjYCvgDustXu9X6vnPxOVckpwBzWRVbSz1m5y/3c78A2ur7zb3F/rcP93u3v3iq5JXbpWofrsm9yPy26PCtbabdbaImttMfAGrp8LqPp12ImrCSGuzHZHMsbE4wrtj6y1X7s362ciCE4J7tlAR/cd8QTgUuD7CJcppIwxDY0xjUseA+cAi3F9zpI74VcD37kffw9c5b6b3gPY4/4KOQ44xxjT1P2V+hz3tmgUks/ufm2vMaaHu533Kq9zOV5JULn9EdfPBbiuw6XGmERjTFugI64bbn5/X9w11CnAn93He19TR3H/f3oLWGatfd7rJf1MBCPSd0dL/uG6a7wS193yeyNdnjB8vna47v4vAJaUfEZc7ZKTgFXARKCZe7vBtWTcamARkO51rutw3ajKBK6N9GcL8vN/gqsZoABXe+P1ofzsQDquwFsNvIx7cJnT/lVwHT5wf86FuALqKK/973V/phV49Yqo6PfF/XM2y319vgASI/2ZK7gOZ+JqBlkIzHf/G1wffyaq808jJ0VEooxTmkpERCRICm4RkSij4BYRiTIKbhGRKKPgFhGJMgpuEZEoo+AWEYkyCm4RkSjz/5HJSoYkVzxoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchtext.data import BucketIterator, interleave_keys\n",
    "\n",
    "device=\"cuda\"\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "model.init()\n",
    "\n",
    "train_iter = BucketIterator(dataset = model._data, batch_size=32, train=True, shuffle=True, \n",
    "                              sort_key=lambda x: interleave_keys(len(x.src), len(x.trg)))\n",
    "all_iter_count = 0\n",
    "\n",
    "#if(graphview):\n",
    "    #fig.show()\n",
    "losses = []\n",
    "running_loss = 0.0\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    \n",
    "    for i, data in enumerate(train_iter):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data.src, data.trg\n",
    "        # override <pad> tokens\n",
    "        #labels = torch.where(labels == pad_id, torch.ones(labels.shape, dtype=labels.dtype) * blank_id, labels)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # try ignoring inner label (<s>), by adding it into ignore_index label (blank)\n",
    "        ignore_id = outer_id # model.output_field.vocab.stoi['</s>']\n",
    "        # blank_id = model.output_field.vocab.stoi['<blank>']\n",
    "        ignore_labels = torch.where(labels == ignore_id, torch.ones(labels.shape, dtype=labels.dtype).to(device) * blank_id, labels)\n",
    "\n",
    "#        raise Exception(inputs.shape, labels.shape)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # preventing weird data: only accept labels with correct expanding dimensions, otherwise throw KeyError\n",
    "        # layer = 1 is the encoder's output, hence number of application is -1\n",
    "        label_dim_dict = {(2 ** v - 1): v-1 for v in [3, 4, 5]}\n",
    "        # forward + backward + optimize\n",
    "        #print(label_dim_dict[labels.shape[0]])\n",
    "        outputs = model(inputs, num_apply_layer=label_dim_dict[labels.shape[0]])\n",
    "        #processed_labels = self.output_field(labels)\n",
    "        # Criterion accept [batch, length] and [batch, vocab, length] respectively\n",
    "        outputs = outputs.transpose(1, 2)\n",
    "        #raise Exception(outputs.shape, labels.shape)\n",
    "        loss = criterion(outputs, ignore_labels) # use ignore_labels to remove inner label\n",
    "        loss.backward()\n",
    "        # forgot gradient clipping. Fixing.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        losses.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        all_iter_count += 1\n",
    "        if (all_iter_count+1) % 1000 == 0:    # print every 1000 mini-batches\n",
    "            print('[%2d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, all_iter_count + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            if(graphview):\n",
    "                plt.clf()\n",
    "                plt.plot(losses)\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', ',', 'the', 'a', \"'\"], ['world', 'our', 'one', 'as', 'is'], ['world', 'as', 'case', '<s>', 'one'], ['world', 'our', 'one', 'as', 'term'], ['spacious', 'big', 'job', 'everyone', 'solution'], ['big', 'spacious', 'solution', 'question', 'breakfast'], ['world', 'case', 'term', 'rich', 'people'], ['world', 'our', 'the', 'is', 'one'], ['world', 'case', '<s>', 'as', 'one'], ['big', 'solution', 'spacious', 'question', 'breakfast'], ['world', 'case', 'term', 'bank', 'largest'], ['big', 'spacious', 'solution', 'question', 'breakfast'], ['world', 'case', 'term', 'bank', 'rich'], ['<s>', 'the', 'a', ',', 'and'], ['<s>', 'really', ',', 'too', 'also']]\n"
     ]
    }
   ],
   "source": [
    "# test the finished model\n",
    "model.eval()\n",
    "\n",
    "test_input = [\"the world is big\".split()]\n",
    "proc_input = model.input_field.process(test_input).to(device)\n",
    "proc_input.shape\n",
    "\n",
    "unproc_output = model(proc_input)\n",
    "# note that the output is the logits version (not softmaxed)\n",
    "proc_output = torch.nn.functional.log_softmax(unproc_output, dim=-1)\n",
    "k=5\n",
    "topk_probs, topk_id = torch.topk(proc_output, k=k, dim=-1)\n",
    "topk_prbs = topk_probs[:, 0]\n",
    "topk_toks = [[model.output_field.vocab.itos[tok] for tok in tokens] for tokens in topk_id[:, 0]] # remove the single dim\n",
    "print(topk_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.000972747802734375, 'world world'), (-0.013129711151123047, 'world big world world'), (-0.01325082778930664, 'world world big world world'), (-0.013383865356445312, 'world big world world'), (-0.025540828704833984, 'world big world big world world')]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#print (model.output_field.vocab.stoi['<s>'], model.output_field.vocab.stoi['</s>'])\n",
    "\n",
    "prbs = topk_prbs.tolist()\n",
    "toks = topk_toks\n",
    "def load_node(index):\n",
    "    # filter unwanted tokens\n",
    "    return [(prob, s) for prob, s in zip(prbs[index], toks[index]) if s != '<s>' and s != \"</s>\"]\n",
    " \n",
    "def best_per_node(parent, leftchild, rightchild, k=5):\n",
    "    # combine left and right\n",
    "    combined = [((lp + rp)/2, ls + ' ' + rs) for lp, ls in leftchild for rp, rs in rightchild]\n",
    "    # all probabilities & corresponding str\n",
    "    alls = parent + combined\n",
    "    best_zipped = sorted(alls, key=lambda x:x[0], reverse=True)[:k]\n",
    "    return best_zipped\n",
    "\n",
    "# start with the lowest, and go up from there\n",
    "node7 = load_node(7); node8 = load_node(8); node9 = load_node(9); node10 = load_node(10);\n",
    "node11 = load_node(11); node12 = load_node(12); node13 = load_node(13); node14 = load_node(14);\n",
    "# 3-7-8, etc.\n",
    "node3 = best_per_node(load_node(3), node7, node8)\n",
    "node4 = best_per_node(load_node(4), node9, node10)\n",
    "node5 = best_per_node(load_node(5), node11, node12)\n",
    "node6 = best_per_node(load_node(6), node13, node14)\n",
    "# 1-3-4, 2-5-6\n",
    "node1 = best_per_node(load_node(1), node3, node4)\n",
    "node2 = best_per_node(load_node(2), node5, node6)\n",
    "# 0-1-2\n",
    "node0 = best_per_node(load_node(0), node1, node2)\n",
    "\n",
    "print(node0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
