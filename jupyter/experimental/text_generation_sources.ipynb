{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with LSTM\n",
    "\n",
    "This notebook contains the code samples found in Chapter 8, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "[...]\n",
    "\n",
    "## Implementing character-level LSTM text generation\n",
    "\n",
    "\n",
    "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a \n",
    "language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this \n",
    "example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model \n",
    "we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the \n",
    "English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus sentences: 999999\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import io, unicodedata, string\n",
    "\n",
    "#path = keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "path = \"/workspace/khoai23/opennmt/data/monolingual/vi/vie_wikipedia_2016_1M-sentences.txt\"\n",
    "text = io.open(path, \"r\").readlines()\n",
    "print('Corpus sentences:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000e+00, 1.00000e+00, 5.00000e+00, 1.40000e+01, 3.50000e+01,\n",
       "        2.10000e+02, 1.03300e+03, 2.40180e+04, 1.60633e+05, 8.14049e+05]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "VN_chars = \"ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂ ưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ\"\n",
    "assert 'a' < 'z'\n",
    "valid_char = lambda x: 'a' < x.lower() < 'z' or x in VN_chars or x in string.punctuation\n",
    "def valid_perc(paragraph):\n",
    "    paragraph = unicodedata.normalize('NFKC', paragraph)\n",
    "    num_chars = len(paragraph)\n",
    "    num_valid_chars = sum( (1 if valid_char(c) else 0 for c in paragraph) )\n",
    "    return float(num_valid_chars) / float(num_chars)\n",
    "\n",
    "truetext = (l.split(\" \", 1)[-1].strip() for l in text if l.strip() != \"\")\n",
    "percs = [valid_perc(l) for l in truetext]\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(percs, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132709\tChức vụ Giám đốc Nhà máy điện hạt nhân Đầu Tiên được trao cho Nikolaev N.A. (Николаев Николай Андреевич).\n",
      "\n",
      "Chức vụ Giám đốc Nhà máy điện hạt nhân Đầu Tiên được trao cho Nikolaev N.A. (Николаев Николай Андреевич).\n"
     ]
    }
   ],
   "source": [
    "nan_percs = [np.nan if p<0.73 else p for p in percs]\n",
    "worst_sent_idx = np.nanargmin(nan_percs)\n",
    "print(text[worst_sent_idx])\n",
    "print(text[worst_sent_idx].split(\"\\t\", 1)[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 36339., 154317., 209381., 170427., 143797., 113447.,  77526.,\n",
       "         56139.,  30885.,   6492.]),\n",
       " array([ 11. ,  35.4,  59.8,  84.2, 108.6, 133. , 157.4, 181.8, 206.2,\n",
       "        230.6, 255. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_text = (line for p, line in zip(percs, text) if p >= 0.7)\n",
    "normalized_text = [l.split(\"\\t\", 1)[-1].strip().lower() for l in filtered_text if l.strip() != \"\"]\n",
    "plt.hist([len(l) for l in normalized_text], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -n '800000,800020p' /workspace/khoai23/opennmt/data/monolingual/vi/vie_wikipedia_2016_1M-sentences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will extract partially-overlapping sequences of length `maxlen`, one-hot encode them and pack them in a 3D Numpy array `x` of \n",
    "shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot \n",
    "encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 25612874\n",
      "Unique characters: 802\n",
      "(25612874, 30) (25612874,)\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 30\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "#for i in range(0, len(text) - maxlen, step):\n",
    "#    sentences.append(text[i: i + maxlen])\n",
    "#    next_chars.append(text[i + maxlen])\n",
    "for l in normalized_text:\n",
    "    for i in range(0, len(l)-maxlen, step):\n",
    "        sentences.append(l[i: i + maxlen])\n",
    "        next_chars.append(l[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "from collections import Counter\n",
    "all_chars = Counter()\n",
    "for l in normalized_text:\n",
    "    all_chars.update(l)\n",
    "# take only characters that have occurred more than 20 times\n",
    "chars = {i+1:c for i, (c, occ) in enumerate(all_chars.most_common()) if occ >= 20}\n",
    "chars[0] = None\n",
    "char_indices = {char:i for i, char in chars.items()}\n",
    "chars[0] = '?'\n",
    "\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "\n",
    "# encode to characters index for input and output\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "x = np.array([[char_indices.get(c, 0) for c in sent] for sent in sentences])\n",
    "y = np.array([char_indices.get(c, 0) for c in next_chars])\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "#plt.hist([occ for c, occ in all_chars.items()], bins=20)\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "#print('Vectorization...')\n",
    "#x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "#y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "#for i, sentence in enumerate(sentences):\n",
    "#    for t, char in enumerate(sentence):\n",
    "#        x[i, t, char_indices[char]] = 1\n",
    "#    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Our network is a single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. But let us note that \n",
    "recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in \n",
    "recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Enforce dynamic session for Keras. Run to avoid GPU ram hogging\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 30, 128)           102656    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 802)               103458    \n",
      "=================================================================\n",
      "Total params: 337,698\n",
      "Trainable params: 337,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Original Sequential, fast and reliable\n",
    "from keras import layers, Model\n",
    "\n",
    "#model = keras.models.Sequential()\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedded = layers.Embedding(input_dim=len(chars), output_dim=128)(inputs)\n",
    "sequenced = layers.LSTM(128, unroll=True)(embedded)\n",
    "outputs = layers.Dense(len(chars), activation='softmax')(sequenced)\n",
    "label_inputs = layers.Input(shape=())\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:~~\n",
    "\n",
    "Our target is single category entropy, thus we will use `tf.keras.losses.SparseCategoricalCrossentropy` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import losses\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "def custom_fn(y_true, y_preds):\n",
    "    y_true = tf.cast(y_true, tf.int32) # hack, reconvert value back to int\n",
    "    y_true_onehot = tf.one_hot(y_true, len(chars), name=\"y_true_category\")\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true_onehot, y_preds)\n",
    "#import inspect\n",
    "#inspect.signature(model.compile)\n",
    "model.compile(loss=custom_fn, optimizer=optimizer) # keras.backend.sparse_categorical_crossentropy, target_tensors=label_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the language model and sampling from it\n",
    "\n",
    "\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, \n",
    "and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures \n",
    "after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of \n",
    "temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "13866496/25612874 [===============>..............] - ETA: 29:33 - loss: 2.2519"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=512,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    sentence_seed = random.choice(normalized_text)\n",
    "    start_index = random.randint(0, len(sentence_seed) - maxlen - 1)\n",
    "    generated_text = sentence_seed[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = [char_indices.get(c, 0) for c in generated_text]\n",
    "\n",
    "            preds = model.predict(np.reshape(sampled, [1, maxlen]), verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            sampled = sampled[1:] + [next_index]\n",
    "\n",
    "            #generated_text += next_char\n",
    "            #generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in \n",
    "particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text \n",
    "becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \n",
    "\"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings \n",
    "of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment \n",
    "with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and \n",
    "realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is \n",
    "sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is \n",
    "a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To \n",
    "evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like \n",
    "our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic \n",
    "statistical structure, thus making it impossible to learn a language model like we just did.\n",
    "\n",
    "\n",
    "## Take aways\n",
    "\n",
    "* We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
    "* In the case of text, such a model is called a \"language model\" and could be based on either words or characters.\n",
    "* Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
    "* One way to handle this is the notion of _softmax temperature_. Always experiment with different temperatures to find the \"right\" one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
