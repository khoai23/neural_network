{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing PCA on book data\n",
    "We will use TFIDF with english stopwords to process the book data, then collapse the matrices using TruncatedSVD; the model would then be physically saved for later conversion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abraham_Lincoln.txt\t\t     John_Ruskin.txt\r\n",
      "Agatha_Christie.txt\t\t     John_Stuart_Mill.txt\r\n",
      "Albert_Einstein.txt\t\t     Jonathan_Swift.txt\r\n",
      "Aldous_Huxley.txt\t\t     Joseph_Conrad.txt\r\n",
      "Alexander_Pope.txt\t\t     Leigh_Hunt.txt\r\n",
      "Alfred_Russel_Wallace.txt\t     Lewis_Carroll.txt\r\n",
      "Ambrose_Bierce.txt\t\t     Lord_Byron.txt\r\n",
      "Andrew_Lang.txt\t\t\t     Lord_Tennyson.txt\r\n",
      "Anthony_Trollope.txt\t\t     Louisa_May_Alcott.txt\r\n",
      "Arnold_Joseph_Toynbee.txt\t     Lucy_Maud_Montgomery.txt\r\n",
      "Baronness_Orczy.txt\t\t     Lyman_Frank_Baum.txt\r\n",
      "Beatrix_Potter.txt\t\t     Mark_Twain.txt\r\n",
      "Benjamin_Disraeli.txt\t\t     Mary_Shelley.txt\r\n",
      "Benjamin_Franklin.txt\t\t     Mary_Stewart_Daggett.txt\r\n",
      "Bertrand_Russell.txt\t\t     Michael_Faraday.txt\r\n",
      "Bram_Stoker.txt\t\t\t     Nathaniel_Hawthorne.txt\r\n",
      "Bret_Harte.txt\t\t\t     O_Henry.txt\r\n",
      "Charles_Darwin.txt\t\t     Oscar_Wilde.txt\r\n",
      "Charles_Dickens.txt\t\t     P_B_Shelley.txt\r\n",
      "Charles_Kingsley.txt\t\t     Percival_Lowell.txt\r\n",
      "Charlotte_Bronte.txt\t\t     P_G_Wodehouse.txt\r\n",
      "Charlotte_Mary_Yonge.txt\t     Philip_Kindred_Dick.txt\r\n",
      "Daniel_Defoe.txt\t\t     Rafael_Sabatini.txt\r\n",
      "D_H_Lawrence.txt\t\t     Ralph_Waldo_Emerson.txt\r\n",
      "Edgar_Allan_Poe.txt\t\t     Richard_Brinsley_Sheridan.txt\r\n",
      "Edgar_Rice_Burroughs.txt\t     R_M_Ballantyne.txt\r\n",
      "Edmund_Burke.txt\t\t     Robert_Browning.txt\r\n",
      "Edward_Phillips_Oppenheim.txt\t     Robert_Burns.txt\r\n",
      "Edward_Stratemeyer.txt\t\t     Robert_Frost.txt\r\n",
      "Elizabeth_Barrett_Browning.txt\t     Robert_Hooke.txt\r\n",
      "Emily_Bronte.txt\t\t     Robert_Louis_Stevenson.txt\r\n",
      "Eugene_O_Neill.txt\t\t     Robert_Southey.txt\r\n",
      "Ezra_Pound.txt\t\t\t     Rudyard_Kipling.txt\r\n",
      "Frank_Richard_Stockton.txt\t     Samuel_Taylor_Coleridge.txt\r\n",
      "George_Alfred_Henty.txt\t\t     Sinclair_Lewis.txt\r\n",
      "George_Bernard_Shaw.txt\t\t     Sir_Arthur_Conan_Doyle.txt\r\n",
      "George_Eliot.txt\t\t     Sir_Francis_Galton.txt\r\n",
      "George_Washington.txt\t\t     Sir_Humphry_Davy.txt\r\n",
      "G_K_Chesterton.txt\t\t     Sir_Isaac_Newton.txt\r\n",
      "Grant_Allen.txt\t\t\t     Sir_Joseph_Dalton_Hooker.txt\r\n",
      "Hamlin_Garland.txt\t\t     Sir_Richard_Francis_Burton.txt\r\n",
      "Harold_Bindloss.txt\t\t     Sir_Walter_Scott.txt\r\n",
      "Harriet_Elizabeth_Beecher_Stowe.txt  Sir_William_Schwenck_Gilbert.txt\r\n",
      "Hector_Hugh_Munro.txt\t\t     Sir_Winston_Churchill.txt\r\n",
      "Henry_David_Thoreau.txt\t\t     Stephen_Leacock.txt\r\n",
      "Henry_Francis_Cary.txt\t\t     Thomas_Carlyle.txt\r\n",
      "Henry_James.txt\t\t\t     Thomas_Crofton_Croker.txt\r\n",
      "Henry_Rider_Haggard.txt\t\t     Thomas_Hardy.txt\r\n",
      "Herbert_George_Wells.txt\t     Thomas_Henry_Huxley.txt\r\n",
      "Herbert_Spencer.txt\t\t     Thomas_Robert_Malthus.txt\r\n",
      "Herman_Melville.txt\t\t     Thornton_Waldo_Burgess.txt\r\n",
      "Howard_Pyle.txt\t\t\t     T_S_Eliot.txt\r\n",
      "Isaac_Asimov.txt\t\t     Ulysses_Grant.txt\r\n",
      "Jack_London.txt\t\t\t     Virginia_Woolf.txt\r\n",
      "Jacob_Abbott.txt\t\t     Walter_de_la_Mare.txt\r\n",
      "James_Bowker.txt\t\t     Walt_Whitman.txt\r\n",
      "James_Fenimore_Cooper.txt\t     Washington_Irving.txt\r\n",
      "James_Joyce.txt\t\t\t     Wilkie_Collins.txt\r\n",
      "James_Matthew_Barrie.txt\t     William_Blake.txt\r\n",
      "James_Otis.txt\t\t\t     William_Butler_Yeats.txt\r\n",
      "James_Russell_Lowell.txt\t     William_Dean_Howells.txt\r\n",
      "Jane_Austen.txt\t\t\t     William_Ewart_Gladstone.txt\r\n",
      "Jerome_Klapka_Jerome.txt\t     William_Henry_Hudson.txt\r\n",
      "John_Bunyan.txt\t\t\t     William_J_Long.txt\r\n",
      "John_Dryden.txt\t\t\t     William_Makepeace_Thackeray.txt\r\n",
      "John_Galsworthy.txt\t\t     William_Penn.txt\r\n",
      "John_Keats.txt\t\t\t     William_Somerset_Maugham.txt\r\n",
      "John_Locke.txt\t\t\t     William_Wordsworth.txt\r\n",
      "John_Maynard_Keynes.txt\t\t     William_Wymark_Jacobs.txt\r\n",
      "John_Milton.txt\t\t\t     Winston_Churchill.txt\r\n",
      "John_Morley.txt\t\t\t     Zane_Grey.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls /workspace/khoai23/opennmt/data/monolingual/Gutenberg/author_compiled\n",
    "#! head /workspace/khoai23/opennmt/data/monolingual/Gutenberg/author_compiled/Abraham_Lincoln.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences read:  11401766\n"
     ]
    }
   ],
   "source": [
    "import io, os\n",
    "data_location = \"/workspace/khoai23/opennmt/data/monolingual/Gutenberg/author_compiled\"\n",
    "all_sentences = []\n",
    "for root, dirs, files in os.walk(data_location):\n",
    "    for f in files:\n",
    "        true_file_path = os.path.join(root, f)\n",
    "        with io.open(true_file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            for paragraph in infile.readlines():\n",
    "                all_sentences.extend(sent_tokenize(paragraph))\n",
    "print(\"Sentences read: \", len(all_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data using default library\n",
    "We will use `TfidfVectorizer` and `TruncatedSVD` to convert data to sparse and downsize the data to a lower dimension respectively. Some arguments:\n",
    "- For TfidfVectorizer: `stop_word=\"english\", min_df=10, max_df=0.98, ngram_range=(1, 3)`\n",
    "- ~~For TruncatedSVD: `n_components=vector_size, algorithm='arpack', n_iter=20, random_state=100`. The algorithm 'arpack' is crucial in stopping the `MemoryError` from being thrown.~~ TruncatedSVD had caused `MemoryError` in both format, currently unusable\n",
    "- For IncrementalPCA: `n_components=vector_size, batch_size=128`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=10, max_df=0.98, ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd. Currently not working!\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "vector_size = 512\n",
    "svd_model = TruncatedSVD(n_components=vector_size, algorithm=\"arpack\", n_iter=20, random_state=100)\n",
    "reduced_matrix = svd_model.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1a70b9222d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpca_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIncrementalPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreduced_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/environment/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environment/lib/python3.5/site-packages/sklearn/decomposition/incremental_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_variance_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environment/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    515\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                                       accept_large_sparse=accept_large_sparse)\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environment/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    319\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "# An alternative: IncrementalPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "vector_size = 512\n",
    "pca_model = IncrementalPCA(n_components=vector_size, batch_size=128)\n",
    "reduced_matrix = pca_model.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can either export the data in a pickle/json object. Note that the values in json might be very large due to its readable nature, compare to pickle's binary serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_path = \"/workspace/khoai23/opennmt/data/monolingual/Gutenberg/svd_vector.pickle\"\n",
    "with io.open(pickle_path, \"wb\") as picklefile:\n",
    "    pickle.dump({\"sentences\": all_sentences, \"vector\": reduced_matrix}, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_path = \"/workspace/khoai23/opennmt/data/monolingual/Gutenberg/svd_vector.json\"\n",
    "with io.open(json_path, \"w\") as jsonfile:\n",
    "    json.dump({\"sentences\": all_sentences, \"vector\": reduced_matrix}, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative version\n",
    "As we intend to load the data into a keras model anyway, it might be better that we save the data with an vocabulary and convert the sentences into their indexed format instead. This would possibly decrease the physical storage size at the expense of unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "unk_threshold = 10\n",
    "\n",
    "counter = Counter()\n",
    "word_ver_sentences = []\n",
    "for sent in all_sentences:\n",
    "    words = sent.strip().split()\n",
    "    counter.update(words)\n",
    "    word_ver_sentences.append(words)\n",
    "valid_words = {k for k, v in counter.items() if v >= unk_threshold}\n",
    "idx_dict = {k: i for i, k in enumerate([\"unk\"] + list(valid_words))}\n",
    "idx_ver_sentences = [[idx_dict.get(w, 0) for w in words] for words in word_ver_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_path = \"/workspace/khoai23/opennmt/data/monolingual/Gutenberg/svd_with_vocab.pickle\"\n",
    "with io.open(pickle_path, \"wb\") as picklefile:\n",
    "    pickle.dump({\"idx_sents\": idx_ver_sentences, \n",
    "                 \"vector\": reduced_matrix, \n",
    "                 \"vocab\"=idx_dict}, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_path = \"/workspace/khoai23/opennmt/data/monolingual/Gutenberg/svd_with_vocab.json\"\n",
    "with io.open(json_path, \"w\") as jsonfile:\n",
    "    json.dump({\"idx_sents\": idx_ver_sentences, \n",
    "                 \"vector\": reduced_matrix, \n",
    "                 \"vocab\"=idx_dict}, jsonfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
